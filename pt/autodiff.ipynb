{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 Os autores do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introdução aos gradientes e diferenciação automática"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Veja no TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar caderno</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Diferenciação Automática e Gradientes\n",
        "\n",
        "[A diferenciação automática](https://en.wikipedia.org/wiki/Automatic_differentiation) é útil para implementar algoritmos de aprendizado de máquina, como [retropropagação](https://en.wikipedia.org/wiki/Backpropagation) , para treinar redes neurais.\n",
        "\n",
        "Neste guia, discutiremos maneiras de calcular gradientes com o TensorFlow, especialmente na execução antecipada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Gradientes de computação\n",
        "\n",
        "Para diferenciar automaticamente, o TensorFlow precisa lembrar quais operações acontecem em qual ordem durante o *encaminhamento* . Em seguida, durante a *passagem para trás* , o TensorFlow percorre essa lista de operações na ordem inversa para calcular gradientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Fitas de gradiente\n",
        "\n",
        "O TensorFlow fornece a API [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) para diferenciação automática; isto é, calcular o gradiente de uma computação em relação a algumas entradas, geralmente `tf.Variable` s. O TensorFlow \"grava\" as operações relevantes executadas dentro do contexto de um `tf.GradientTape` em uma \"fita\". O TensorFlow usa essa fita para calcular os gradientes de uma computação \"gravada\" usando [a diferenciação de modo reverso](https://en.wikipedia.org/wiki/Automatic_differentiation) .\n",
        "\n",
        "Aqui está um exemplo simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "Depois de registrar algumas operações, use `GradientTape.gradient(target, sources)` para calcular o gradiente de algum destino (geralmente uma perda) em relação a alguma fonte (geralmente as variáveis do modelo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "LsvrwF6bHroC"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "O exemplo acima usa escalares, mas `tf.GradientTape` funciona facilmente em qualquer tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [

      ],
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "Para obter o gradiente de `y` em relação a ambas as variáveis, você pode passar ambas como fontes para o método `gradient` . A fita é flexível sobre como as fontes são passadas e aceitará qualquer combinação aninhada de listas ou dicionários e retornará o gradiente estruturado da mesma maneira (consulte `tf.nest` )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "luOtK1Da_BR0"
      },
      "outputs": [

      ],
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "O gradiente em relação a cada fonte tem a forma da fonte:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "aYbWRFPZqk4U"
      },
      "outputs": [

      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Aqui está o cálculo do gradiente novamente, desta vez passando um dicionário de variáveis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "d73cY6NOuaMd"
      },
      "outputs": [

      ],
      "source": [
        "my_vars = {\n",
        "    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),\n",
        "    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Gradientes em relação a um modelo\n",
        "\n",
        "É comum coletar `tf.Variables` em um `tf.Module` ou em uma de suas subclasses ( `layers.Layer` , `keras.Model` ) para [verificação](checkpoint.ipynb) e [exportação](saved_model.ipynb) .\n",
        "\n",
        "Na maioria dos casos, você desejará calcular gradientes em relação às variáveis treináveis de um modelo. Como todas as subclasses de `tf.Module` agregam suas variáveis na propriedade `Module.trainable_variables` , você pode calcular esses gradientes em poucas linhas de código: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JvesHtbQESc-"
      },
      "outputs": [

      ],
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PR_ezr6UFrpI"
      },
      "outputs": [

      ],
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Controlando o que a fita assiste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "O comportamento padrão é gravar todas as operações após acessar uma `tf.Variable` treinável. As razões para isso são:\n",
        "\n",
        "- A fita precisa saber quais operações gravar na passagem para frente para calcular os gradientes na passagem para trás.\n",
        "- A fita contém referências a saídas intermediárias, portanto, você não deseja gravar operações desnecessárias.\n",
        "- O caso de uso mais comum envolve o cálculo do gradiente de uma perda em relação a todas as variáveis treináveis de um modelo.\n",
        "\n",
        "Por exemplo, o seguinte falha ao calcular um gradiente porque o `tf.Tensor` não é \"observado\" por padrão e o `tf.Variable` não é treinável:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Kj9gPckdB37a"
      },
      "outputs": [

      ],
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "Você pode listar as variáveis que estão sendo observadas pela fita usando o método `GradientTape.watched_variables` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "hwNwjW1eAkib"
      },
      "outputs": [

      ],
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` fornece ganchos que dão ao usuário controle sobre o que é ou não observado.\n",
        "\n",
        "Para gravar gradientes em relação a um `tf.Tensor` , você precisa chamar `GradientTape.watch(x)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tVN1QqFRDHBK"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "Por outro lado, para desabilitar o comportamento padrão de observar todos os `tf.Variables` , defina `watch_accessed_variables=False` ao criar a fita gradiente. Este cálculo usa duas variáveis, mas apenas conecta o gradiente para uma das variáveis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7QPzwWvSEwIp"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Como `GradientTape.watch` não foi chamado em `x0` , nenhum gradiente é calculado em relação a ele:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e6GM-3evH1Sz"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Resultados intermediários\n",
        "\n",
        "Você também pode solicitar gradientes da saída em relação a valores intermediários calculados dentro do contexto `tf.GradientTape` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7XaPRAwUyYms"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "# Use the tape to compute the gradient of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dx = 2 * y, where y = x ** 2\n",
        "print(tape.gradient(z, y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "Por padrão, os recursos mantidos por um `GradientTape` são liberados assim que o método `GradientTape.gradient()` é chamado. Para calcular vários gradientes no mesmo cálculo, crie uma fita de gradiente `persistent` . Isso permite várias chamadas para o método `gradient()` à medida que os recursos são liberados quando o objeto de fita é coletado como lixo. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant([1, 3.0])\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
        "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "j8bv_jQFg6CN"
      },
      "outputs": [

      ],
      "source": [
        "del tape   # Drop the reference to the tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Notas sobre o desempenho\n",
        "\n",
        "- Há uma pequena sobrecarga associada à execução de operações dentro de um contexto de fita de gradiente. Para a execução mais rápida, isso não será um custo perceptível, mas você ainda deve usar o contexto de fita nas áreas apenas onde for necessário.\n",
        "\n",
        "- As fitas de gradiente usam memória para armazenar resultados intermediários, incluindo entradas e saídas, para uso durante a passagem para trás.\n",
        "\n",
        "    Para eficiência, algumas operações (como `ReLU` ) não precisam manter seus resultados intermediários e são podadas durante o passe para frente. No entanto, se você usar `persistent=True` em sua fita, *nada será descartado* e seu pico de uso de memória será maior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Gradientes de alvos não escalares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pldU9F5duP2"
      },
      "source": [
        "Um gradiente é fundamentalmente uma operação em um escalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "qI0sDV_WeXBb"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient(y0, x).numpy())\n",
        "print(tape.gradient(y1, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Assim, se você solicitar o gradiente de vários alvos, o resultado para cada fonte será:\n",
        "\n",
        "- O gradiente da soma dos alvos, ou equivalentemente\n",
        "- A soma dos gradientes de cada alvo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "o4a6_YOcfWKS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Da mesma forma, se os alvos não forem escalares, o gradiente da soma é calculado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DArPWqsSh5un"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "Isso simplifica a obtenção do gradiente da soma de uma coleção de perdas ou o gradiente da soma de um cálculo de perda por elemento.\n",
        "\n",
        "Se você precisar de um gradiente separado para cada item, veja [Jacobians](advanced_autodiff.ipynb#jacobians) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iwFswok8RAly"
      },
      "source": [
        "Em alguns casos, você pode pular o jacobiano. Para um cálculo elementar, o gradiente da soma fornece a derivada de cada elemento em relação ao seu elemento de entrada, uma vez que cada elemento é independente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JQvk_jnMmTDS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e_f2QgDPmcPE"
      },
      "outputs": [

      ],
      "source": [
        "plt.plot(x, y, label='y')\n",
        "plt.plot(x, dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Controle de fluxo\n",
        "\n",
        "Como as fitas gravam as operações à medida que são executadas, o fluxo de controle do Python (usando `if` s e `while` s, por exemplo) é manipulado naturalmente.\n",
        "\n",
        "Aqui uma variável diferente é usada em cada ramificação de um `if` . O gradiente só se conecta à variável que foi usada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ciFLizhrrjy7"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1**2 \n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Apenas lembre-se de que as próprias instruções de controle não são diferenciáveis, portanto, são invisíveis para otimizadores baseados em gradiente.\n",
        "\n",
        "Dependendo do valor de `x` no exemplo acima, a fita grava `result = v0` ou `result = v1**2` . O gradiente em relação a `x` é sempre `None` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8k05WmuAwPm7"
      },
      "outputs": [

      ],
      "source": [
        "dx = tape.gradient(result, x)\n",
        "\n",
        "print(dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Obtendo um gradiente de `None`\n",
        "\n",
        "Quando um destino não está conectado a uma fonte, você obterá um gradiente de `None` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CU185WDM81Ut"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(z, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Aqui `z` obviamente não está conectado a `x` , mas existem várias maneiras menos óbvias de desconectar um gradiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Substituiu uma variável por um tensor.\n",
        "\n",
        "Na seção sobre [\"controlar o que a fita assiste\"](#watches) , você viu que a fita assistirá automaticamente a um `tf.Variable` mas não a um `tf.Tensor` .\n",
        "\n",
        "Um erro comum é substituir inadvertidamente um `tf.Variable` por um `tf.Tensor` , em vez de usar `Variable.assign` para atualizar o `tf.Variable` . Aqui está um exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "QPKY4Tn9zX7_"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x+1\n",
        "\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Fez cálculos fora do TensorFlow\n",
        "\n",
        "A fita não pode gravar o caminho do gradiente se o cálculo sair do TensorFlow. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jmoLCDJb_yw1"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Tomou gradientes por meio de um inteiro ou string\n",
        "\n",
        "Inteiros e strings não são diferenciáveis. Se um caminho de cálculo usar esses tipos de dados, não haverá gradiente.\n",
        "\n",
        "Ninguém espera que as strings sejam diferenciáveis, mas é fácil criar acidentalmente uma constante ou variável `int` se você não especificar o `dtype` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "9jlHXHqfASU3"
      },
      "outputs": [

      ],
      "source": [
        "# The x0 variable has an `int` dtype.\n",
        "x = tf.Variable([[2, 2],\n",
        "                 [2, 2]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # The path to x1 is blocked by the `int` dtype here.\n",
        "  y = tf.cast(x, tf.float32)\n",
        "  y = tf.reduce_sum(x)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "O TensorFlow não converte automaticamente entre os tipos, portanto, na prática, você geralmente receberá um erro de tipo em vez de um gradiente ausente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 5. Levou gradientes por meio de um objeto com estado\n",
        "\n",
        "O estado pára gradientes. Quando você lê de um objeto com estado, a fita só pode ver o estado atual, não o histórico que leva a ele.\n",
        "\n",
        "Um `tf.Tensor` é imutável. Você não pode alterar um tensor depois de criado. Tem um *valor* , mas nenhum *estado* . Todas as operações discutidas até agora também são stateless: A saída de um `tf.matmul` depende apenas de suas entradas.\n",
        "\n",
        "Uma `tf.Variable` tem um estado interno, seu valor. Quando você usa a variável, o estado é lido. É normal calcular um gradiente em relação a uma variável, mas o estado da variável impede que os cálculos de gradiente voltem mais para trás. Por exemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "C1tLeeRFE479"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Update x1 = x1 + x0.\n",
        "  x1.assign_add(x0)\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Da mesma forma, os iteradores `tf.data.Dataset` e `tf.queue` s são stateful e pararão todos os gradientes nos tensores que passam por eles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## Nenhum gradiente registrado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Alguns `tf.Operation` s são **registrados como não diferenciáveis** e retornarão `None` . Outros **não possuem gradiente registrado** .\n",
        "\n",
        "A página [tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops) mostra quais operações de baixo nível têm gradientes registrados.\n",
        "\n",
        "Se você tentar obter um gradiente por meio de uma operação flutuante que não tenha gradiente registrado, a fita gerará um erro em vez de retornar silenciosamente `None` . Dessa forma, você sabe que algo deu errado.\n",
        "\n",
        "Por exemplo, a função `tf.image.adjust_contrast` envolve [raw_ops.AdjustContrastv2](https://www.tensorflow.org/api_docs/python/tf/raw_ops#.AdjustContrastv2) que pode ter um gradiente, mas o gradiente não é implementado:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HSb20FXc_V0U"
      },
      "outputs": [

      ],
      "source": [
        "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "delta = tf.Variable(0.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta]))\n",
        "  assert False   # This should not happen.\n",
        "except LookupError as e:\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDoutjzATiEm"
      },
      "source": [
        "Se você precisar diferenciar por meio dessa operação, precisará implementar o gradiente e registrá-lo (usando `tf.RegisterGradient` ), ou reimplementar a função usando outras operações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Zeros em vez de Nenhum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYDrVogA89eA"
      },
      "source": [
        "Em alguns casos, seria conveniente obter 0 em vez de `None` para gradientes não conectados. Você pode decidir o que retornar quando tiver gradientes desconectados usando o argumento `unconnected_gradients` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
